{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_logical_devices())\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, f1_score, accuracy_score,rand_score\n",
    "from sklearn.cluster import MeanShift, KMeans\n",
    "from sklearn import cluster\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.stats import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "\n",
    "x_train=np.load(\"data\\\\numpy\\\\x_train.npy\")\n",
    "y_train=np.load(\"data\\\\numpy\\\\y_train.npy\")\n",
    "p_train=np.load(\"data\\\\numpy\\\\p_train.npy\",allow_pickle=True)\n",
    "\n",
    "x_val=np.load(\"data\\\\numpy\\\\x_val.npy\")\n",
    "y_val=np.load(\"data\\\\numpy\\\\y_val.npy\")\n",
    "p_val=np.load(\"data\\\\numpy\\\\p_val.npy\",allow_pickle=True)\n",
    "\n",
    "x_test=np.load(\"data\\\\numpy\\\\x_test.npy\")\n",
    "y_test=np.load(\"data\\\\numpy\\\\y_test.npy\")\n",
    "p_test=np.load(\"data\\\\numpy\\\\p_test.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def build_encoder(input_dim, latent_dim):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim,), name='input')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    z = tf.keras.layers.Dense(latent_dim, activation='linear', name='z')(x)\n",
    "    return tf.keras.Model(inputs, z, name='encoder')\n",
    "\n",
    "def build_classifier(latent_dim, num_emotions):\n",
    "    latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='latent_input')\n",
    "    x = tf.keras.layers.Dense(8, activation='relu')(latent_inputs)\n",
    "    outputs = tf.keras.layers.Dense(num_emotions, activation='softmax', name='output')(x)\n",
    "    return tf.keras.Model(latent_inputs, outputs, name='classifier')\n",
    "\n",
    "\n",
    "def build_model(input_dim, latent_dim, num_emotions):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim,), name='input')\n",
    "    encoder = build_encoder(input_dim, latent_dim)\n",
    "    z = encoder(inputs)\n",
    "    classifier = build_classifier(latent_dim, num_emotions)\n",
    "    classifier_output = classifier(z)\n",
    "    return tf.keras.Model(inputs, [z, classifier_output], name='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_kmeans_plus_plus(X, n_clusters):\n",
    "    centroids = [X[np.random.randint(len(X))]]\n",
    "    while len(centroids) < n_clusters:\n",
    "        distances = np.array([min([np.linalg.norm(x - c) for c in centroids]) for x in X])\n",
    "        probabilities = distances / distances.sum()\n",
    "        cumprobs = probabilities.cumsum()\n",
    "        r = np.random.rand()\n",
    "        for j, p in enumerate(cumprobs):\n",
    "            if r < p:\n",
    "                i = j\n",
    "                break\n",
    "        centroids.append(X[i])\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, n_clusters,fixed_centroids, max_iter=300, tol=0.0001):\n",
    "    X=np.array(X)\n",
    "    centroids = initialize_kmeans_plus_plus(X, n_clusters)\n",
    "    if len(fixed_centroids)>0:\n",
    "      centroids = np.vstack((fixed_centroids, centroids))\n",
    "\n",
    "    n_clusters=len(centroids)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1)\n",
    "        np.unique(labels)\n",
    "        np.seterr(all='ignore')\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
    "        # Handle possible empty clusters\n",
    "        for idx in range(n_clusters):\n",
    "            if np.isnan(new_centroids[idx]).any():\n",
    "                new_centroids[idx] = centroids[idx]\n",
    "        \n",
    "        if len(fixed_centroids)>0:\n",
    "          new_centroids[:len(fixed_centroids)] = fixed_centroids\n",
    "\n",
    "        if np.linalg.norm(centroids - new_centroids) < tol:\n",
    "                break\n",
    "\n",
    "        centroids = new_centroids\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting_data_kmeans_fixed(x_test_train,y_test_train,model_aux, fixed_c,mod=\"z\"):\n",
    "   if mod==\"z\":\n",
    "      z,res=model_aux(x_test_train, training=False)\n",
    "\n",
    "      if len(fixed_c)>0:\n",
    "         z_fixed,res_fixed=model_aux(fixed_c, training=False)\n",
    "      else:\n",
    "         z_fixed=[]\n",
    "\n",
    "      cs,res_kmeans=kmeans(z,5,z_fixed)\n",
    "\n",
    "   best_x=[]\n",
    "   best_y=[]\n",
    "   for i in range(len(cs)):\n",
    "      arg=np.argmin(np.sum(np.square(z-cs[i]),axis=1))\n",
    "      best_x.append(x_test_train[arg])\n",
    "      best_y.append(y_test_train[arg])\n",
    "   return np.array(best_x),np.array(best_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting_data_kmeans_fixed_1(x_test_train,y_test_train,model_aux,model_aux_prev, fixed_c,mod=\"z\"):\n",
    "   if mod==\"z\":\n",
    "      z,res=model_aux(x_test_train, training=False)\n",
    "      z_prev,_=model_aux_prev(x_test_train, training=False)\n",
    "      z=np.concatenate([z,z_prev-z,res],axis=1)\n",
    "\n",
    "      max=np.max(z,axis=0)\n",
    "      min=np.min(z,axis=0)\n",
    "\n",
    "      z= (z-min)/(max-min)\n",
    "\n",
    "      if len(fixed_c)>0:\n",
    "         z_fixed,res_fixed=model_aux(fixed_c, training=False)\n",
    "         z_fixed_prev,_=model_aux_prev(fixed_c, training=False)\n",
    "         z_fixed=np.concatenate([z_fixed,z_fixed_prev-z_fixed,res_fixed],axis=1)\n",
    "\n",
    "         z_fixed= (z_fixed-min)/(max-min)\n",
    "\n",
    "\n",
    "      else:\n",
    "         z_fixed=[]\n",
    "\n",
    "      cs,res_kmeans=kmeans(z,5,z_fixed)\n",
    "\n",
    "   best_x=[]\n",
    "   best_y=[]\n",
    "   for i in range(len(cs)):\n",
    "        arg=np.argmin(np.sum(np.square(z-cs[i]),axis=1))\n",
    "        best_x.append(x_test_train[arg])\n",
    "        best_y.append(y_test_train[arg])\n",
    "\n",
    "   return np.array(best_x),np.array(best_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the original model\n",
    "def main():\n",
    "  optimizer=tf.keras.optimizers.Adam()\n",
    "\n",
    "  def classification_loss(classifier_outputs, labels, z_mean,unique_labels, lambda_var=0.1, lambda_inter=0.1):\n",
    "      classification_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels, classifier_outputs), axis=-1)\n",
    "      total_loss = classification_loss\n",
    "      labels = tf.argmax(labels, axis=-1)\n",
    "\n",
    "      # Calcular la penalización para la varianza intra-grupo\n",
    "      intra_group_penalty =  tf.constant(0,dtype=\"float32\")\n",
    "\n",
    "      n_unique_labels = len(unique_labels)\n",
    "\n",
    "      for i in range(n_unique_labels):\n",
    "          if len(z_mean[labels == unique_labels[i]]) > 1:\n",
    "              intra_group_penalty += tf.reduce_sum(tf.square(tf.math.reduce_std(z_mean[labels == unique_labels[i]], axis=0)))\n",
    "\n",
    "      # Calcular la penalización para maximizar las distancias entre los centroides de los grupos\n",
    "      inter_group_penalty = tf.constant(0,dtype=\"float32\")\n",
    "      if n_unique_labels > 1:\n",
    "          for i in range(n_unique_labels):\n",
    "              for j in range(i + 1, n_unique_labels):\n",
    "                  inter_group_penalty += tf.reduce_sum(tf.square(tf.reduce_mean(z_mean[labels == unique_labels[i]], axis=0) - tf.reduce_mean(z_mean[labels == unique_labels[j]], axis=0)))\n",
    "\n",
    "      # Agregar ambas penalizaciones a la pérdida total\n",
    "      total_loss += lambda_var * intra_group_penalty + lambda_inter * 1/inter_group_penalty\n",
    "      return total_loss\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(model, inputs,labels, unique_labels):\n",
    "      with tf.GradientTape() as tape:\n",
    "          z, pred = model(inputs, training=True)\n",
    "          loss=classification_loss(pred, labels, z, unique_labels )\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "      return loss\n",
    "\n",
    "  def val_step(model, inputs, labels, unique_labels):\n",
    "      z, pred= model(inputs, training=False)\n",
    "      loss=classification_loss(pred, labels, z, unique_labels)\n",
    "      return loss\n",
    "\n",
    "  model_aux=tf.keras.models.clone_model(model)\n",
    "  pesos=model.get_weights()\n",
    "  model_aux.set_weights(pesos)\n",
    "  accs=[]\n",
    "\n",
    "  x_min, x_max = -3, 3\n",
    "  y_min, y_max = -3, 3\n",
    "\n",
    "  for step in range(number_of_steps+1): #number of steps\n",
    "\n",
    "    #create a copy of cls layers to predict the classifier borders\n",
    "    new_model = Sequential([\n",
    "      model_aux.get_layer(\"classifier\").layers[0],  # Dense layer 1\n",
    "      model_aux.get_layer(\"classifier\").layers[1],  # Dense layer 2\n",
    "      model_aux.get_layer(\"classifier\").layers[2]   # Output layer\n",
    "    ])\n",
    "    new_model.set_weights(model_aux.get_layer(\"classifier\").get_weights())\n",
    "\n",
    "    #Plot contour Test\n",
    "    if cls_borders+test_contour+train_contour>0:\n",
    "      plt.figure(figsize=(7,7))\n",
    "    if test_contour==True:\n",
    "      z_test,_=model_aux(x_test_train)\n",
    "      for i in range(5):\n",
    "          # Filtrar datos por clase\n",
    "          z_class = z_test[np.argmax(y_test_train, axis=1) == i]\n",
    "\n",
    "          # Calcular el histograma bidimensional\n",
    "          hist, xedges, yedges = np.histogram2d(z_class[:,0], z_class[:,1], bins=100, range=[[x_min, x_max], [y_min, y_max]])\n",
    "\n",
    "          # Normalizar el histograma\n",
    "          hist_normalized = hist.T\n",
    "\n",
    "          # Aplicar suavizado gaussiano\n",
    "          hist_smooth = gaussian_filter(hist_normalized, sigma=3)\n",
    "\n",
    "          # Ajustar distribución normal al histograma\n",
    "          hist_flat = hist_smooth.flatten()\n",
    "          mu, std = norm.fit(hist_flat)\n",
    "\n",
    "          # Calcular los percentiles correspondientes al 90% y 95%\n",
    "          percentile_70 = norm.ppf(0.70, loc=mu, scale=std)\n",
    "          percentile_80 = norm.ppf(0.80, loc=mu, scale=std)\n",
    "          percentile_90 = norm.ppf(0.90, loc=mu, scale=std)\n",
    "\n",
    "          upper_90_value = norm.sf(percentile_90, loc=mu, scale=std)\n",
    "          upper_80_value = norm.sf(percentile_80, loc=mu, scale=std)\n",
    "          upper_70_value = norm.sf(percentile_70, loc=mu, scale=std)\n",
    "\n",
    "          # Definir los niveles de confianza para las curvas de nivel\n",
    "          contour_levels = [upper_90_value]\n",
    "\n",
    "          # Crear malla para las curvas de nivel\n",
    "          xx = np.linspace(x_min, x_max, hist.shape[0])\n",
    "          yy = np.linspace(y_min, y_max, hist.shape[1])\n",
    "          XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "          # Tramar las curvas de nivel\n",
    "          plt.contour(XX, YY, hist_smooth, colors=colors[i], levels=contour_levels, linewidths=1)\n",
    "\n",
    "    #Plot contour Train\n",
    "    if train_contour==True:\n",
    "      z_train,_=model_aux(x_train)\n",
    "\n",
    "      for i in range(5):\n",
    "          # Filtrar datos por clase\n",
    "          z_class = z_train[np.argmax(y_train, axis=1) == i]\n",
    "\n",
    "          # Calcular el histograma bidimensional\n",
    "          hist, xedges, yedges = np.histogram2d(z_class[:,0], z_class[:,1], bins=100, range=[[x_min, x_max], [y_min, y_max]])\n",
    "\n",
    "          # Aplicar suavizado gaussiano\n",
    "          hist_smooth = gaussian_filter(hist.T, sigma=3)\n",
    "\n",
    "          # Ajustar distribución normal al histograma\n",
    "          hist_flat = hist_smooth.flatten()\n",
    "          mu, std = norm.fit(hist_flat)\n",
    "\n",
    "          # Calcular los percentiles correspondientes al 90%\n",
    "          percentile_90 = norm.ppf(0.90, loc=mu, scale=std)\n",
    "          upper_90_value = norm.sf(percentile_90, loc=mu, scale=std)\n",
    "\n",
    "          # Definir los niveles de confianza para las curvas de nivel\n",
    "          contour_levels = [upper_90_value]\n",
    "\n",
    "          # Crear malla para las curvas de nivel\n",
    "          xx = np.linspace(x_min, x_max, hist.shape[0])\n",
    "          yy = np.linspace(y_min, y_max, hist.shape[1])\n",
    "          XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "          # Tramar las curvas de nivel\n",
    "          plt.contour(XX, YY, hist_smooth, colors=colors[i], levels=contour_levels, linewidths=2, linestyles='dashed')\n",
    "\n",
    "    #selecting samples\n",
    "\n",
    "    #kmeans_fixed\n",
    "\n",
    "    if selection_method==\"kmeans_fixed\":\n",
    "      if step==0:\n",
    "        samples_x,samples_y=selecting_data_kmeans_fixed(x_test_train,y_test_train,model_aux,[], mod=\"z\")\n",
    "      else:\n",
    "        samples_x,samples_y=selecting_data_kmeans_fixed(x_test_train,y_test_train,model_aux,best_x, mod=\"z\")\n",
    "    \n",
    "    if selection_method==\"random\":\n",
    "      if step==0:\n",
    "        selected=np.random.randint(0, len(x_test_train),5)\n",
    "        selected=selected.reshape(-1,1)\n",
    "      else:\n",
    "        selected=np.vstack([selected,np.random.randint(0, len(x_test_train),5).reshape(-1,1)])\n",
    "      \n",
    "      selected=selected.reshape(-1,1)\n",
    "      #print(selected)\n",
    "      samples_x,samples_y=x_test_train[selected.reshape(-1)],y_test_train[selected.reshape(-1)]\n",
    "    \n",
    "    if selection_method==\"kmeans_fixed_mov\":\n",
    "        if step==0:\n",
    "          samples_x,samples_y=selecting_data_kmeans_fixed(x_test_train,y_test_train,model_aux,[], mod=\"z\")\n",
    "        else:\n",
    "          samples_x,samples_y=selecting_data_kmeans_fixed_1(x_test_train,y_test_train,model_aux,model_prev,best_x, mod=\"z\")\n",
    "\n",
    "\n",
    "    z_selected,_=model_aux(samples_x)\n",
    "    best_x=samples_x\n",
    "    best_y=samples_y\n",
    "    centroids,_ = model_aux(best_x)\n",
    "\n",
    "    #condition to plot only 5 repetitions\n",
    "    if step==number_of_steps:\n",
    "      z_selected=z_selected[:-number_of_steps]\n",
    "\n",
    "    #map the borders\n",
    "    if cls_borders==True:\n",
    "      def predict_cluster(point, centroids):\n",
    "          distances = np.linalg.norm(point-centroids, axis=-1)\n",
    "          return np.argmin(distances,axis=0)\n",
    "\n",
    "      xx= np.arange(x_min, x_max, 0.1)\n",
    "      Z = np.zeros((60,60))\n",
    "      Z1= np.zeros((60,60))\n",
    "      for i in range(len(xx)):\n",
    "            for j in range(len(xx)):\n",
    "                Z[j, i] = predict_cluster([xx[i], xx[j]], centroids)\n",
    "                aux=tf.convert_to_tensor(np.array([[xx[i], xx[j]]]),dtype=tf.float32)\n",
    "                Z1[j, i] = np.argmax(new_model(aux)[0])\n",
    "\n",
    "      for u in range(5):\n",
    "          plt.contour(xx, xx, Z1==u,alpha=0.25,linewidths=2,linestyles=\"solid\", colors=\"grey\")\n",
    "\n",
    "      for iz,z in enumerate(z_selected.numpy()):\n",
    "        plt.scatter(z[0],z[1], c=colors[np.argmax(samples_y,axis=1)][iz], alpha=0.3, marker=\"o\",s=275)\n",
    "        plt.text(z[0],z[1],str((np.ceil(iz/5+0.2)-1).astype(int)), color=colors[np.argmax(samples_y,axis=1)][iz],fontsize=18,horizontalalignment='center',verticalalignment='center', alpha=1)\n",
    "\n",
    "\n",
    "    model_prev=tf.keras.models.clone_model(model)\n",
    "    pesos=model_aux.get_weights()\n",
    "    model_prev.set_weights(pesos)\n",
    "\n",
    "    #retrain model\n",
    "    for epoch in range(200):\n",
    "        random=np.random.choice(np.arange(0,len(x_train)),len(best_x)*2)\n",
    "        x= np.concatenate([best_x, x_train[random]])\n",
    "        y= np.concatenate([best_y, y_train[random]])\n",
    "        train_step(model_aux,x,y, np.unique(np.argmax(y,axis=1)))\n",
    "\n",
    "    #evaluation\n",
    "    z,res=model_aux(x_val)\n",
    "    acc_val=balanced_accuracy_score(np.argmax(y_val,axis=1),np.argmax(res,axis=1))\n",
    "\n",
    "    z,res=model_aux(x_test_test)\n",
    "    acc_test=balanced_accuracy_score(np.argmax(y_test_test,axis=1),np.argmax(res,axis=1))\n",
    "\n",
    "\n",
    "\n",
    "    if cls_borders+test_contour+train_contour>0:\n",
    "      plt.xlabel(\"z1\")\n",
    "      plt.ylabel(\"z2\")\n",
    "      plt.xlim(-2.5,1.5)\n",
    "      plt.ylim(-2.5,3)\n",
    "      plt.grid()\n",
    "      plt.show()\n",
    "    accs.append((acc_val,acc_test))\n",
    "  return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18532\\923006619.py:13: RuntimeWarning: Mean of empty slice.\n",
      "  new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18532\\923006619.py:13: RuntimeWarning: Mean of empty slice.\n",
      "  new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(n_clusters)])\n",
      "100%|██████████| 2/2 [01:28<00:00, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.8393221573830443, 0.5661843942391054), (0.8478475969405123, 0.580748781146561), (0.8564137004433828, 0.6169834010656001), (0.859086653529993, 0.6293263755758801), (0.8677432491144254, 0.6447663441795859), (0.8588021719975858, 0.6863757157242026)], [(0.8373328869339145, 0.5582301521511248), (0.8631121932138044, 0.6070810523631985), (0.8652420996891381, 0.6190210092390613), (0.8676194022816676, 0.6506621769050737), (0.864923731142319, 0.6534162723854808), (0.8665237311423191, 0.6681981697826814)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model=build_model(1024,4,5)\n",
    "model.load_weights(\"weights\\\\4D\\\\mlp_4d_3\")\n",
    "\n",
    "person=\"0001\"\n",
    "x_test_person=x_test[p_test==person]\n",
    "y_test_person=y_test[p_test==person]\n",
    "x_test_train,x_test_test, y_test_train,y_test_test=train_test_split(x_test_person, y_test_person, test_size=0.3, random_state=123)\n",
    "\n",
    "\n",
    "cls_borders=False\n",
    "test_contour=False\n",
    "train_contour=False\n",
    "colors=np.array(['orange','black','green','blue','purple'])\n",
    "selection_method=\"kmeans_fixed\" #random, kmeans_fixed,kmeans_fixed_mov\n",
    "number_of_steps=5\n",
    "rep=2\n",
    "\n",
    "accs=[]\n",
    "for r in tqdm.tqdm(range(rep)):\n",
    "    accs.append(main())\n",
    "print(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
